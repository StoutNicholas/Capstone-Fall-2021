{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "8fe40e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "7f7c908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_260 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 36)                468       \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 6)                 222       \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 781\n",
      "Trainable params: 781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up activation, hidden, and output layers.\n",
    "model = Sequential([\n",
    "    Dense(units = 12, input_shape = (6,), activation = 'relu'),\n",
    "    Dense(units = 36, activation = 'relu'),\n",
    "    Dense(units = 6, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Layer summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "f6ef96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cochise county model\n",
    "coconino = pd.read_csv(\"C:/Users/edoar/Documents/School_Documents/Fall_2021/Capstone/random_forest_files/coconino.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "5a525853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>flagstaff_hmi</th>\n",
       "      <th>total_arrests</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>population_estimates</th>\n",
       "      <th>poverty_percentage</th>\n",
       "      <th>state_tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>10434</td>\n",
       "      <td>6660.4</td>\n",
       "      <td>116717</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>11375</td>\n",
       "      <td>6848.8</td>\n",
       "      <td>117803</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>11638</td>\n",
       "      <td>7717.6</td>\n",
       "      <td>120390</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>12876</td>\n",
       "      <td>7502.3</td>\n",
       "      <td>121545</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>14102</td>\n",
       "      <td>6702.4</td>\n",
       "      <td>123349</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12666</td>\n",
       "      <td>6972.5</td>\n",
       "      <td>124804</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>13067</td>\n",
       "      <td>6586.8</td>\n",
       "      <td>126029</td>\n",
       "      <td>16.8</td>\n",
       "      <td>11981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>11691</td>\n",
       "      <td>6099.8</td>\n",
       "      <td>127451</td>\n",
       "      <td>16.2</td>\n",
       "      <td>14844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>12442</td>\n",
       "      <td>5838.6</td>\n",
       "      <td>128426</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>10494</td>\n",
       "      <td>5183.9</td>\n",
       "      <td>129849</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>11638</td>\n",
       "      <td>5196.9</td>\n",
       "      <td>134612</td>\n",
       "      <td>23.8</td>\n",
       "      <td>15256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>13067</td>\n",
       "      <td>5386.6</td>\n",
       "      <td>134275</td>\n",
       "      <td>21.6</td>\n",
       "      <td>19877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>12442</td>\n",
       "      <td>10148.5</td>\n",
       "      <td>136146</td>\n",
       "      <td>24.6</td>\n",
       "      <td>21458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>11638</td>\n",
       "      <td>9798.9</td>\n",
       "      <td>136699</td>\n",
       "      <td>23.2</td>\n",
       "      <td>20524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>13067</td>\n",
       "      <td>10101.2</td>\n",
       "      <td>137566</td>\n",
       "      <td>21.3</td>\n",
       "      <td>17736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>10434</td>\n",
       "      <td>9567.6</td>\n",
       "      <td>138962</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>11638</td>\n",
       "      <td>9882.2</td>\n",
       "      <td>140407</td>\n",
       "      <td>17.8</td>\n",
       "      <td>19961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>12666</td>\n",
       "      <td>9621.3</td>\n",
       "      <td>141001</td>\n",
       "      <td>18.4</td>\n",
       "      <td>21138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>10494</td>\n",
       "      <td>8909.8</td>\n",
       "      <td>142523</td>\n",
       "      <td>16.8</td>\n",
       "      <td>22857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>13067</td>\n",
       "      <td>9191.2</td>\n",
       "      <td>143476</td>\n",
       "      <td>15.9</td>\n",
       "      <td>21138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>11638</td>\n",
       "      <td>6972.5</td>\n",
       "      <td>137566</td>\n",
       "      <td>18.4</td>\n",
       "      <td>21458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>12666</td>\n",
       "      <td>6586.8</td>\n",
       "      <td>140407</td>\n",
       "      <td>23.2</td>\n",
       "      <td>22857000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  flagstaff_hmi  total_arrests  crime_rate  population_estimates  \\\n",
       "0   2000              0          10434      6660.4                116717   \n",
       "1   2001              0          11375      6848.8                117803   \n",
       "2   2002              0          11638      7717.6                120390   \n",
       "3   2003              1          12876      7502.3                121545   \n",
       "4   2004              1          14102      6702.4                123349   \n",
       "5   2005              1          12666      6972.5                124804   \n",
       "6   2006              1          13067      6586.8                126029   \n",
       "7   2007              1          11691      6099.8                127451   \n",
       "8   2008              1          12442      5838.6                128426   \n",
       "9   2009              1          10494      5183.9                129849   \n",
       "10  2010              1          11638      5196.9                134612   \n",
       "11  2011              1          13067      5386.6                134275   \n",
       "12  2012              1          12442     10148.5                136146   \n",
       "13  2013              1          11638      9798.9                136699   \n",
       "14  2014              1          13067     10101.2                137566   \n",
       "15  2015              1          10434      9567.6                138962   \n",
       "16  2016              1          11638      9882.2                140407   \n",
       "17  2017              1          12666      9621.3                141001   \n",
       "18  2018              1          10494      8909.8                142523   \n",
       "19  2019              1          13067      9191.2                143476   \n",
       "20  2020              1          11638      6972.5                137566   \n",
       "21  2021              1          12666      6586.8                140407   \n",
       "\n",
       "    poverty_percentage  state_tax  \n",
       "0                 15.0    7303000  \n",
       "1                 15.0    8545000  \n",
       "2                 15.0   11246000  \n",
       "3                 15.0   11981000  \n",
       "4                 16.0    8545000  \n",
       "5                 18.0    8545000  \n",
       "6                 16.8   11981000  \n",
       "7                 16.2   14844000  \n",
       "8                 16.0   12448000  \n",
       "9                 18.0   14844000  \n",
       "10                23.8   15256000  \n",
       "11                21.6   19877000  \n",
       "12                24.6   21458000  \n",
       "13                23.2   20524000  \n",
       "14                21.3   17736000  \n",
       "15                19.5   19334000  \n",
       "16                17.8   19961000  \n",
       "17                18.4   21138000  \n",
       "18                16.8   22857000  \n",
       "19                15.9   21138000  \n",
       "20                18.4   21458000  \n",
       "21                23.2   22857000  "
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coconino['flagstaff_hmi'] = pd.cut(coconino.flagstaff_hmi, bins = [0, 199999, 600000], labels = ['0', '1'])\n",
    "coconino['flagstaff_hmi'] = coconino['flagstaff_hmi'].astype(int)\n",
    "coconino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "feed4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = coconino.drop('flagstaff_hmi', axis = 1)\n",
    "Y = coconino['flagstaff_hmi']\n",
    "\n",
    "# Splitting training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "aaabdf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 2.6910 - accuracy: 0.8235 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "history = model.fit(x = X_train, y = y_train.values, batch_size = 4, epochs = 100, shuffle = True, verbose = 2, validation_data = (X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "e877cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.36\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, accuracy = model.evaluate(X, Y, verbose = 0)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "a1f03e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A1E242FDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e403bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9d337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a220d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "588ac9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_264 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_265 (Dense)            (None, 36)                468       \n",
      "_________________________________________________________________\n",
      "dense_266 (Dense)            (None, 6)                 222       \n",
      "_________________________________________________________________\n",
      "dense_267 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 781\n",
      "Trainable params: 781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up activation, hidden, and output layers.\n",
    "model = Sequential([\n",
    "    Dense(units = 12, input_shape = (6,), activation = 'relu'),\n",
    "    Dense(units = 36, activation = 'relu'),\n",
    "    Dense(units = 6, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Layer summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "2d4656bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gila county model\n",
    "gila = pd.read_csv(\"C:/Users/edoar/Documents/School_Documents/Fall_2021/Capstone/random_forest_files/gila.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "2dfe8da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>payson_hmi</th>\n",
       "      <th>total_arrests</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>population_estimates</th>\n",
       "      <th>poverty_percentage</th>\n",
       "      <th>state_tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>3840</td>\n",
       "      <td>4443.4</td>\n",
       "      <td>51355</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>5406</td>\n",
       "      <td>4280.4</td>\n",
       "      <td>51165</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>4492</td>\n",
       "      <td>4564.5</td>\n",
       "      <td>51225</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>3845</td>\n",
       "      <td>4347.7</td>\n",
       "      <td>50906</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>4074</td>\n",
       "      <td>4266.2</td>\n",
       "      <td>50848</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>4205</td>\n",
       "      <td>4368.8</td>\n",
       "      <td>50914</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>4174</td>\n",
       "      <td>4023.3</td>\n",
       "      <td>51635</td>\n",
       "      <td>20.0</td>\n",
       "      <td>27391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>3849</td>\n",
       "      <td>3216.0</td>\n",
       "      <td>52231</td>\n",
       "      <td>18.2</td>\n",
       "      <td>24820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>4565</td>\n",
       "      <td>3603.5</td>\n",
       "      <td>52273</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>4519</td>\n",
       "      <td>5604.3</td>\n",
       "      <td>52199</td>\n",
       "      <td>20.7</td>\n",
       "      <td>27391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>4074</td>\n",
       "      <td>5321.2</td>\n",
       "      <td>53561</td>\n",
       "      <td>19.8</td>\n",
       "      <td>23779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>4174</td>\n",
       "      <td>4058.4</td>\n",
       "      <td>53440</td>\n",
       "      <td>25.7</td>\n",
       "      <td>25343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>3849</td>\n",
       "      <td>8827.8</td>\n",
       "      <td>52994</td>\n",
       "      <td>21.3</td>\n",
       "      <td>25365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>4174</td>\n",
       "      <td>11717.9</td>\n",
       "      <td>53005</td>\n",
       "      <td>22.6</td>\n",
       "      <td>24275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>4492</td>\n",
       "      <td>10448.8</td>\n",
       "      <td>53044</td>\n",
       "      <td>24.4</td>\n",
       "      <td>20977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>3840</td>\n",
       "      <td>10350.5</td>\n",
       "      <td>52978</td>\n",
       "      <td>21.3</td>\n",
       "      <td>23257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>3849</td>\n",
       "      <td>11082.9</td>\n",
       "      <td>53356</td>\n",
       "      <td>20.3</td>\n",
       "      <td>24820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3840</td>\n",
       "      <td>16600.3</td>\n",
       "      <td>53578</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5406</td>\n",
       "      <td>11949.1</td>\n",
       "      <td>53801</td>\n",
       "      <td>25.7</td>\n",
       "      <td>22202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>4174</td>\n",
       "      <td>12702.2</td>\n",
       "      <td>54018</td>\n",
       "      <td>21.1</td>\n",
       "      <td>24820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3845</td>\n",
       "      <td>16600.3</td>\n",
       "      <td>53005</td>\n",
       "      <td>25.7</td>\n",
       "      <td>18905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>3849</td>\n",
       "      <td>11949.1</td>\n",
       "      <td>53801</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14919000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  payson_hmi  total_arrests  crime_rate  population_estimates  \\\n",
       "0   2000           0           3840      4443.4                 51355   \n",
       "1   2001           0           5406      4280.4                 51165   \n",
       "2   2002           0           4492      4564.5                 51225   \n",
       "3   2003           0           3845      4347.7                 50906   \n",
       "4   2004           0           4074      4266.2                 50848   \n",
       "5   2005           0           4205      4368.8                 50914   \n",
       "6   2006           0           4174      4023.3                 51635   \n",
       "7   2007           0           3849      3216.0                 52231   \n",
       "8   2008           0           4565      3603.5                 52273   \n",
       "9   2009           0           4519      5604.3                 52199   \n",
       "10  2010           0           4074      5321.2                 53561   \n",
       "11  2011           0           4174      4058.4                 53440   \n",
       "12  2012           0           3849      8827.8                 52994   \n",
       "13  2013           0           4174     11717.9                 53005   \n",
       "14  2014           0           4492     10448.8                 53044   \n",
       "15  2015           0           3840     10350.5                 52978   \n",
       "16  2016           0           3849     11082.9                 53356   \n",
       "17  2017           0           3840     16600.3                 53578   \n",
       "18  2018           1           5406     11949.1                 53801   \n",
       "19  2019           1           4174     12702.2                 54018   \n",
       "20  2020           1           3845     16600.3                 53005   \n",
       "21  2021           1           3849     11949.1                 53801   \n",
       "\n",
       "    poverty_percentage  state_tax  \n",
       "0                 17.0   14919000  \n",
       "1                 17.0   18905000  \n",
       "2                 17.0   23779000  \n",
       "3                 17.0   25343000  \n",
       "4                 18.0   27391000  \n",
       "5                 19.0   22202000  \n",
       "6                 20.0   27391000  \n",
       "7                 18.2   24820000  \n",
       "8                 16.0   18905000  \n",
       "9                 20.7   27391000  \n",
       "10                19.8   23779000  \n",
       "11                25.7   25343000  \n",
       "12                21.3   25365000  \n",
       "13                22.6   24275000  \n",
       "14                24.4   20977000  \n",
       "15                21.3   23257000  \n",
       "16                20.3   24820000  \n",
       "17                24.1   26739000  \n",
       "18                25.7   22202000  \n",
       "19                21.1   24820000  \n",
       "20                25.7   18905000  \n",
       "21                16.0   14919000  "
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gila['payson_hmi'] = pd.cut(gila.payson_hmi, bins = [0, 199999, 600000], labels = ['0', '1'])\n",
    "gila['payson_hmi'] = gila['payson_hmi'].astype(int)\n",
    "gila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "dbe5aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = gila.drop('payson_hmi', axis = 1)\n",
    "Y = gila['payson_hmi']\n",
    "\n",
    "# Splitting training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "52c8b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 2/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 3/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 4/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 6/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 7/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 8/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 9/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 10/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 11/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 12/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 13/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 14/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 15/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 16/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 17/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 18/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 19/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 20/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 21/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 22/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 23/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 24/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 25/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 26/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 27/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 28/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 29/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 30/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 31/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 32/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 33/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 34/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 35/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 36/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 37/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 38/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 39/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 40/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 41/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 42/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 43/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 44/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 45/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 46/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 47/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 48/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 49/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 50/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 51/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 52/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 53/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 54/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 55/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 56/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 57/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 58/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 59/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 60/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 61/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 62/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 63/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 64/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 65/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 66/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 67/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 68/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 69/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 70/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 71/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 72/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 73/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 74/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 75/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 76/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 77/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 78/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 79/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 80/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 81/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 82/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 83/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 85/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 86/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 87/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 88/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 89/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 90/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 91/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 92/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 93/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 94/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 95/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 96/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 97/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 98/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 99/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 100/100\n",
      "3/3 - 0s - loss: 12.5582 - accuracy: 0.1765 - val_loss: 12.1994 - val_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "history = model.fit(x = X_train, y = y_train.values, batch_size = 8, epochs = 100, shuffle = True, verbose = 2, validation_data = (X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "id": "cbe1997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 18.18\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, accuracy = model.evaluate(X, Y, verbose = 0)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "b8ef5afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A1E8F4DD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8944271909999159"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea30d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49616154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e73ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "id": "3583dfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_268 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_269 (Dense)            (None, 36)                468       \n",
      "_________________________________________________________________\n",
      "dense_270 (Dense)            (None, 6)                 222       \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 781\n",
      "Trainable params: 781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up activation, hidden, and output layers.\n",
    "model = Sequential([\n",
    "    Dense(units = 12, input_shape = (6,), activation = 'relu'),\n",
    "    Dense(units = 36, activation = 'relu'),\n",
    "    Dense(units = 6, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Layer summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "acb09414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graham county model\n",
    "graham = pd.read_csv(\"C:/Users/edoar/Documents/School_Documents/Fall_2021/Capstone/random_forest_files/graham.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "e27c5b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>safford_hmi</th>\n",
       "      <th>total_arrests</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>population_estimates</th>\n",
       "      <th>poverty_percentage</th>\n",
       "      <th>state_tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>1563</td>\n",
       "      <td>2445.6</td>\n",
       "      <td>33541</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>2421.9</td>\n",
       "      <td>33307</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>1378</td>\n",
       "      <td>2428.7</td>\n",
       "      <td>33120</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>1354</td>\n",
       "      <td>2537.2</td>\n",
       "      <td>32785</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>1286</td>\n",
       "      <td>2134.3</td>\n",
       "      <td>32441</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>670</td>\n",
       "      <td>1146.1</td>\n",
       "      <td>32629</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>757</td>\n",
       "      <td>2096.9</td>\n",
       "      <td>33286</td>\n",
       "      <td>23.4</td>\n",
       "      <td>3707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>850</td>\n",
       "      <td>1588.2</td>\n",
       "      <td>34736</td>\n",
       "      <td>22.4</td>\n",
       "      <td>4835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1180</td>\n",
       "      <td>2136.9</td>\n",
       "      <td>36204</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>1369</td>\n",
       "      <td>4777.7</td>\n",
       "      <td>37045</td>\n",
       "      <td>21.5</td>\n",
       "      <td>2596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>1378</td>\n",
       "      <td>1972.1</td>\n",
       "      <td>37152</td>\n",
       "      <td>21.6</td>\n",
       "      <td>3691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>1378</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>37126</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>850</td>\n",
       "      <td>3648.3</td>\n",
       "      <td>37015</td>\n",
       "      <td>25.4</td>\n",
       "      <td>4396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1378</td>\n",
       "      <td>2956.3</td>\n",
       "      <td>37433</td>\n",
       "      <td>21.6</td>\n",
       "      <td>5624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>757</td>\n",
       "      <td>2884.3</td>\n",
       "      <td>38099</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1563</td>\n",
       "      <td>5073.0</td>\n",
       "      <td>37860</td>\n",
       "      <td>22.9</td>\n",
       "      <td>4792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1378</td>\n",
       "      <td>4290.1</td>\n",
       "      <td>37807</td>\n",
       "      <td>22.9</td>\n",
       "      <td>5114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1180</td>\n",
       "      <td>4112.6</td>\n",
       "      <td>37481</td>\n",
       "      <td>20.9</td>\n",
       "      <td>4835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1369</td>\n",
       "      <td>3255.6</td>\n",
       "      <td>37995</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>1354</td>\n",
       "      <td>3967.4</td>\n",
       "      <td>38837</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1354</td>\n",
       "      <td>2096.9</td>\n",
       "      <td>37152</td>\n",
       "      <td>20.1</td>\n",
       "      <td>4792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1378</td>\n",
       "      <td>2134.3</td>\n",
       "      <td>37126</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2596000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  safford_hmi  total_arrests  crime_rate  population_estimates  \\\n",
       "0   2000            0           1563      2445.6                 33541   \n",
       "1   2001            0           1461      2421.9                 33307   \n",
       "2   2002            0           1378      2428.7                 33120   \n",
       "3   2003            0           1354      2537.2                 32785   \n",
       "4   2004            0           1286      2134.3                 32441   \n",
       "5   2005            0            670      1146.1                 32629   \n",
       "6   2006            0            757      2096.9                 33286   \n",
       "7   2007            0            850      1588.2                 34736   \n",
       "8   2008            0           1180      2136.9                 36204   \n",
       "9   2009            0           1369      4777.7                 37045   \n",
       "10  2010            0           1378      1972.1                 37152   \n",
       "11  2011            0           1378      1820.0                 37126   \n",
       "12  2012            0            850      3648.3                 37015   \n",
       "13  2013            0           1378      2956.3                 37433   \n",
       "14  2014            0            757      2884.3                 38099   \n",
       "15  2015            0           1563      5073.0                 37860   \n",
       "16  2016            0           1378      4290.1                 37807   \n",
       "17  2017            0           1180      4112.6                 37481   \n",
       "18  2018            0           1369      3255.6                 37995   \n",
       "19  2019            0           1354      3967.4                 38837   \n",
       "20  2020            0           1354      2096.9                 37152   \n",
       "21  2021            1           1378      2134.3                 37126   \n",
       "\n",
       "    poverty_percentage  state_tax  \n",
       "0                 21.0    4396000  \n",
       "1                 22.0    2596000  \n",
       "2                 22.0    4792000  \n",
       "3                 20.0    2415000  \n",
       "4                 22.0    3691000  \n",
       "5                 23.0    4396000  \n",
       "6                 23.4    3707000  \n",
       "7                 22.4    4835000  \n",
       "8                 21.0    2415000  \n",
       "9                 21.5    2596000  \n",
       "10                21.6    3691000  \n",
       "11                26.0    3707000  \n",
       "12                25.4    4396000  \n",
       "13                21.6    5624000  \n",
       "14                20.0    4860000  \n",
       "15                22.9    4792000  \n",
       "16                22.9    5114000  \n",
       "17                20.9    4835000  \n",
       "18                21.0    5228000  \n",
       "19                20.1    2415000  \n",
       "20                20.1    4792000  \n",
       "21                21.0    2596000  "
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graham['safford_hmi'] = pd.cut(graham.safford_hmi, bins = [0, 199999, 600000], labels = ['0', '1'])\n",
    "graham['safford_hmi'] = graham['safford_hmi'].astype(int)\n",
    "graham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "dc7239f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = graham.drop('safford_hmi', axis = 1)\n",
    "Y = graham['safford_hmi']\n",
    "\n",
    "# Splitting training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "ea7ee844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 15.2492 - accuracy: 0.0000e+00 - val_loss: 12.1994 - val_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "history = model.fit(x = X_train, y = y_train.values, batch_size = 4, epochs = 100, shuffle = True, verbose = 2, validation_data = (X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "e55e39a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 4.55\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, accuracy = model.evaluate(X, Y, verbose = 0)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "2f5b145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A1E3901AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8944271909999159"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f306f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dcd0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4965532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "014572d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_272 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_273 (Dense)            (None, 36)                468       \n",
      "_________________________________________________________________\n",
      "dense_274 (Dense)            (None, 6)                 222       \n",
      "_________________________________________________________________\n",
      "dense_275 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 781\n",
      "Trainable params: 781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up activation, hidden, and output layers.\n",
    "model = Sequential([\n",
    "    Dense(units = 12, input_shape = (6,), activation = 'relu'),\n",
    "    Dense(units = 36, activation = 'relu'),\n",
    "    Dense(units = 6, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Layer summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "7b3becc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maricopa county model\n",
    "maricopa = pd.read_csv(\"C:/Users/edoar/Documents/School_Documents/Fall_2021/Capstone/random_forest_files/maricopa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "28b3a991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>phoenix_hmi</th>\n",
       "      <th>total_arrests</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>population_estimates</th>\n",
       "      <th>poverty_percentage</th>\n",
       "      <th>state_tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>155480</td>\n",
       "      <td>6167.4</td>\n",
       "      <td>3097378</td>\n",
       "      <td>10.0</td>\n",
       "      <td>384937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>149496</td>\n",
       "      <td>6183.1</td>\n",
       "      <td>3200075</td>\n",
       "      <td>11.0</td>\n",
       "      <td>528421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>156126</td>\n",
       "      <td>6402.9</td>\n",
       "      <td>3299127</td>\n",
       "      <td>11.0</td>\n",
       "      <td>442961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>158182</td>\n",
       "      <td>5867.1</td>\n",
       "      <td>3391391</td>\n",
       "      <td>12.0</td>\n",
       "      <td>475144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>160559</td>\n",
       "      <td>5507.9</td>\n",
       "      <td>3502635</td>\n",
       "      <td>13.0</td>\n",
       "      <td>528421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>147950</td>\n",
       "      <td>5080.8</td>\n",
       "      <td>3647131</td>\n",
       "      <td>12.0</td>\n",
       "      <td>554877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>160996</td>\n",
       "      <td>5098.1</td>\n",
       "      <td>3776118</td>\n",
       "      <td>12.5</td>\n",
       "      <td>604492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>174332</td>\n",
       "      <td>5079.7</td>\n",
       "      <td>3872962</td>\n",
       "      <td>12.9</td>\n",
       "      <td>554877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>184138</td>\n",
       "      <td>5004.7</td>\n",
       "      <td>3958263</td>\n",
       "      <td>13.0</td>\n",
       "      <td>510299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>187775</td>\n",
       "      <td>4457.8</td>\n",
       "      <td>4023132</td>\n",
       "      <td>15.1</td>\n",
       "      <td>564781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>156126</td>\n",
       "      <td>4510.1</td>\n",
       "      <td>3825110</td>\n",
       "      <td>16.6</td>\n",
       "      <td>605041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>160559</td>\n",
       "      <td>4604.4</td>\n",
       "      <td>3874996</td>\n",
       "      <td>17.4</td>\n",
       "      <td>598626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>156126</td>\n",
       "      <td>8829.8</td>\n",
       "      <td>3947505</td>\n",
       "      <td>17.4</td>\n",
       "      <td>581109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>160996</td>\n",
       "      <td>8454.8</td>\n",
       "      <td>4017723</td>\n",
       "      <td>17.6</td>\n",
       "      <td>512762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>187775</td>\n",
       "      <td>7992.8</td>\n",
       "      <td>4093648</td>\n",
       "      <td>17.1</td>\n",
       "      <td>475814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>155480</td>\n",
       "      <td>7742.3</td>\n",
       "      <td>4172905</td>\n",
       "      <td>16.3</td>\n",
       "      <td>513252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>156126</td>\n",
       "      <td>8064.9</td>\n",
       "      <td>4256143</td>\n",
       "      <td>15.0</td>\n",
       "      <td>549496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>149496</td>\n",
       "      <td>8253.8</td>\n",
       "      <td>4327184</td>\n",
       "      <td>13.5</td>\n",
       "      <td>592884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>158182</td>\n",
       "      <td>7861.1</td>\n",
       "      <td>4402403</td>\n",
       "      <td>15.1</td>\n",
       "      <td>627439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>174332</td>\n",
       "      <td>7285.6</td>\n",
       "      <td>4485414</td>\n",
       "      <td>12.2</td>\n",
       "      <td>384937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>158182</td>\n",
       "      <td>6402.9</td>\n",
       "      <td>3825110</td>\n",
       "      <td>10.0</td>\n",
       "      <td>598626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>156126</td>\n",
       "      <td>5867.1</td>\n",
       "      <td>3874996</td>\n",
       "      <td>17.4</td>\n",
       "      <td>604492000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  phoenix_hmi  total_arrests  crime_rate  population_estimates  \\\n",
       "0   2000            0         155480      6167.4               3097378   \n",
       "1   2001            0         149496      6183.1               3200075   \n",
       "2   2002            0         156126      6402.9               3299127   \n",
       "3   2003            0         158182      5867.1               3391391   \n",
       "4   2004            0         160559      5507.9               3502635   \n",
       "5   2005            1         147950      5080.8               3647131   \n",
       "6   2006            1         160996      5098.1               3776118   \n",
       "7   2007            1         174332      5079.7               3872962   \n",
       "8   2008            1         184138      5004.7               3958263   \n",
       "9   2009            0         187775      4457.8               4023132   \n",
       "10  2010            0         156126      4510.1               3825110   \n",
       "11  2011            0         160559      4604.4               3874996   \n",
       "12  2012            0         156126      8829.8               3947505   \n",
       "13  2013            0         160996      8454.8               4017723   \n",
       "14  2014            1         187775      7992.8               4093648   \n",
       "15  2015            1         155480      7742.3               4172905   \n",
       "16  2016            1         156126      8064.9               4256143   \n",
       "17  2017            1         149496      8253.8               4327184   \n",
       "18  2018            1         158182      7861.1               4402403   \n",
       "19  2019            1         174332      7285.6               4485414   \n",
       "20  2020            1         158182      6402.9               3825110   \n",
       "21  2021            1         156126      5867.1               3874996   \n",
       "\n",
       "    poverty_percentage  state_tax  \n",
       "0                 10.0  384937000  \n",
       "1                 11.0  528421000  \n",
       "2                 11.0  442961000  \n",
       "3                 12.0  475144000  \n",
       "4                 13.0  528421000  \n",
       "5                 12.0  554877000  \n",
       "6                 12.5  604492000  \n",
       "7                 12.9  554877000  \n",
       "8                 13.0  510299000  \n",
       "9                 15.1  564781000  \n",
       "10                16.6  605041000  \n",
       "11                17.4  598626000  \n",
       "12                17.4  581109000  \n",
       "13                17.6  512762000  \n",
       "14                17.1  475814000  \n",
       "15                16.3  513252000  \n",
       "16                15.0  549496000  \n",
       "17                13.5  592884000  \n",
       "18                15.1  627439000  \n",
       "19                12.2  384937000  \n",
       "20                10.0  598626000  \n",
       "21                17.4  604492000  "
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maricopa['phoenix_hmi'] = pd.cut(maricopa.phoenix_hmi, bins = [0, 199999, 600000], labels = ['0', '1'])\n",
    "maricopa['phoenix_hmi'] = maricopa['phoenix_hmi'].astype(int)\n",
    "maricopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "555e5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = maricopa.drop('phoenix_hmi', axis = 1)\n",
    "Y = maricopa['phoenix_hmi']\n",
    "\n",
    "# Splitting training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "05977bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 3.0498 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "history = model.fit(x = X_train, y = y_train.values, batch_size = 4, epochs = 100, shuffle = True, verbose = 2, validation_data = (X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "a904597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.55\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, accuracy = model.evaluate(X, Y, verbose = 0)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "cd8c91cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A1ED9F78B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4472135954999579"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc05d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162ad73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ab682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "9b7023e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_276 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            (None, 36)                468       \n",
      "_________________________________________________________________\n",
      "dense_278 (Dense)            (None, 6)                 222       \n",
      "_________________________________________________________________\n",
      "dense_279 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 781\n",
      "Trainable params: 781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up activation, hidden, and output layers.\n",
    "model = Sequential([\n",
    "    Dense(units = 12, input_shape = (6,), activation = 'relu'),\n",
    "    Dense(units = 36, activation = 'relu'),\n",
    "    Dense(units = 6, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Layer summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "cb6c4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mohave county model\n",
    "mohave = pd.read_csv(\"C:/Users/edoar/Documents/School_Documents/Fall_2021/Capstone/random_forest_files/mohave.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "98952c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lake_havasu_city_hmi</th>\n",
       "      <th>total_arrests</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>population_estimates</th>\n",
       "      <th>poverty_percentage</th>\n",
       "      <th>state_tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>11459</td>\n",
       "      <td>4936.4</td>\n",
       "      <td>156194</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>11212</td>\n",
       "      <td>5221.6</td>\n",
       "      <td>159840</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>11910</td>\n",
       "      <td>5489.4</td>\n",
       "      <td>165177</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>13953</td>\n",
       "      <td>6344.7</td>\n",
       "      <td>171019</td>\n",
       "      <td>14.0</td>\n",
       "      <td>29325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>15840</td>\n",
       "      <td>6308.2</td>\n",
       "      <td>178313</td>\n",
       "      <td>15.0</td>\n",
       "      <td>32768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>16608</td>\n",
       "      <td>6145.2</td>\n",
       "      <td>185940</td>\n",
       "      <td>15.0</td>\n",
       "      <td>43663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>17255</td>\n",
       "      <td>5580.1</td>\n",
       "      <td>192724</td>\n",
       "      <td>16.0</td>\n",
       "      <td>49790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>16705</td>\n",
       "      <td>5384.3</td>\n",
       "      <td>195852</td>\n",
       "      <td>13.5</td>\n",
       "      <td>54301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>15808</td>\n",
       "      <td>4336.3</td>\n",
       "      <td>195601</td>\n",
       "      <td>17.0</td>\n",
       "      <td>51325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>15855</td>\n",
       "      <td>3984.7</td>\n",
       "      <td>194825</td>\n",
       "      <td>17.8</td>\n",
       "      <td>59585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>15840</td>\n",
       "      <td>3843.1</td>\n",
       "      <td>200314</td>\n",
       "      <td>18.7</td>\n",
       "      <td>60460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>11459</td>\n",
       "      <td>3835.3</td>\n",
       "      <td>202812</td>\n",
       "      <td>21.7</td>\n",
       "      <td>57461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>16608</td>\n",
       "      <td>8885.5</td>\n",
       "      <td>203347</td>\n",
       "      <td>21.1</td>\n",
       "      <td>54301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>15808</td>\n",
       "      <td>8546.1</td>\n",
       "      <td>203126</td>\n",
       "      <td>21.3</td>\n",
       "      <td>51878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>15808</td>\n",
       "      <td>8347.9</td>\n",
       "      <td>203423</td>\n",
       "      <td>20.8</td>\n",
       "      <td>44831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>16608</td>\n",
       "      <td>7664.8</td>\n",
       "      <td>204574</td>\n",
       "      <td>17.4</td>\n",
       "      <td>44884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>17255</td>\n",
       "      <td>8263.4</td>\n",
       "      <td>205412</td>\n",
       "      <td>18.3</td>\n",
       "      <td>47900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>15840</td>\n",
       "      <td>8409.3</td>\n",
       "      <td>207017</td>\n",
       "      <td>17.3</td>\n",
       "      <td>51604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>11459</td>\n",
       "      <td>7259.7</td>\n",
       "      <td>209292</td>\n",
       "      <td>17.0</td>\n",
       "      <td>55799000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>11212</td>\n",
       "      <td>6978.3</td>\n",
       "      <td>212181</td>\n",
       "      <td>16.0</td>\n",
       "      <td>59585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>15840</td>\n",
       "      <td>7664.8</td>\n",
       "      <td>203347</td>\n",
       "      <td>20.8</td>\n",
       "      <td>49790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>15808</td>\n",
       "      <td>6145.2</td>\n",
       "      <td>207017</td>\n",
       "      <td>14.0</td>\n",
       "      <td>54301000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  lake_havasu_city_hmi  total_arrests  crime_rate  \\\n",
       "0   2000                     0          11459      4936.4   \n",
       "1   2001                     0          11212      5221.6   \n",
       "2   2002                     0          11910      5489.4   \n",
       "3   2003                     0          13953      6344.7   \n",
       "4   2004                     0          15840      6308.2   \n",
       "5   2005                     1          16608      6145.2   \n",
       "6   2006                     1          17255      5580.1   \n",
       "7   2007                     1          16705      5384.3   \n",
       "8   2008                     1          15808      4336.3   \n",
       "9   2009                     0          15855      3984.7   \n",
       "10  2010                     0          15840      3843.1   \n",
       "11  2011                     0          11459      3835.3   \n",
       "12  2012                     0          16608      8885.5   \n",
       "13  2013                     0          15808      8546.1   \n",
       "14  2014                     0          15808      8347.9   \n",
       "15  2015                     0          16608      7664.8   \n",
       "16  2016                     0          17255      8263.4   \n",
       "17  2017                     0          15840      8409.3   \n",
       "18  2018                     1          11459      7259.7   \n",
       "19  2019                     1          11212      6978.3   \n",
       "20  2020                     1          15840      7664.8   \n",
       "21  2021                     1          15808      6145.2   \n",
       "\n",
       "    population_estimates  poverty_percentage  state_tax  \n",
       "0                 156194                15.0   26588000  \n",
       "1                 159840                16.0   29325000  \n",
       "2                 165177                15.0   27522000  \n",
       "3                 171019                14.0   29325000  \n",
       "4                 178313                15.0   32768000  \n",
       "5                 185940                15.0   43663000  \n",
       "6                 192724                16.0   49790000  \n",
       "7                 195852                13.5   54301000  \n",
       "8                 195601                17.0   51325000  \n",
       "9                 194825                17.8   59585000  \n",
       "10                200314                18.7   60460000  \n",
       "11                202812                21.7   57461000  \n",
       "12                203347                21.1   54301000  \n",
       "13                203126                21.3   51878000  \n",
       "14                203423                20.8   44831000  \n",
       "15                204574                17.4   44884000  \n",
       "16                205412                18.3   47900000  \n",
       "17                207017                17.3   51604000  \n",
       "18                209292                17.0   55799000  \n",
       "19                212181                16.0   59585000  \n",
       "20                203347                20.8   49790000  \n",
       "21                207017                14.0   54301000  "
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mohave['lake_havasu_city_hmi'] = pd.cut(mohave.lake_havasu_city_hmi, bins = [0, 199999, 600000], labels = ['0', '1'])\n",
    "mohave['lake_havasu_city_hmi'] = mohave['lake_havasu_city_hmi'].astype(int)\n",
    "mohave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "a02a2506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = mohave.drop('lake_havasu_city_hmi', axis = 1)\n",
    "Y = mohave['lake_havasu_city_hmi']\n",
    "\n",
    "# Splitting training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "2c1694c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 8.0731 - accuracy: 0.4706 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "history = model.fit(x = X_train, y = y_train.values, batch_size = 4, epochs = 100, shuffle = True, verbose = 2, validation_data = (X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "fadca29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.36\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, accuracy = model.evaluate(X, Y, verbose = 0)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "577ace47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A1E92D8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7dfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f77cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262394c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "442912ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_280 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_281 (Dense)            (None, 36)                468       \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 6)                 222       \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 781\n",
      "Trainable params: 781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up activation, hidden, and output layers.\n",
    "model = Sequential([\n",
    "    Dense(units = 12, input_shape = (6,), activation = 'relu'),\n",
    "    Dense(units = 36, activation = 'relu'),\n",
    "    Dense(units = 6, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Layer summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "13d9db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navajo county model\n",
    "navajo = pd.read_csv(\"C:/Users/edoar/Documents/School_Documents/Fall_2021/Capstone/random_forest_files/navajo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "6616b246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>show_low_hmi</th>\n",
       "      <th>total_arrests</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>population_estimates</th>\n",
       "      <th>poverty_percentage</th>\n",
       "      <th>state_tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>5893</td>\n",
       "      <td>3433.9</td>\n",
       "      <td>97876</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>6019</td>\n",
       "      <td>3403.9</td>\n",
       "      <td>98549</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>6116</td>\n",
       "      <td>3436.2</td>\n",
       "      <td>101358</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>6021</td>\n",
       "      <td>3797.0</td>\n",
       "      <td>103220</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>6609</td>\n",
       "      <td>3378.5</td>\n",
       "      <td>105020</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>6365</td>\n",
       "      <td>3344.9</td>\n",
       "      <td>106968</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>6612</td>\n",
       "      <td>2926.0</td>\n",
       "      <td>109000</td>\n",
       "      <td>24.4</td>\n",
       "      <td>17950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>6890</td>\n",
       "      <td>2866.7</td>\n",
       "      <td>110999</td>\n",
       "      <td>23.4</td>\n",
       "      <td>12613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>7490</td>\n",
       "      <td>3085.9</td>\n",
       "      <td>112348</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>6771</td>\n",
       "      <td>3053.0</td>\n",
       "      <td>112975</td>\n",
       "      <td>27.4</td>\n",
       "      <td>10935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>6116</td>\n",
       "      <td>3246.2</td>\n",
       "      <td>107697</td>\n",
       "      <td>27.8</td>\n",
       "      <td>12294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>6771</td>\n",
       "      <td>2971.5</td>\n",
       "      <td>107552</td>\n",
       "      <td>32.4</td>\n",
       "      <td>12613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>6116</td>\n",
       "      <td>5841.7</td>\n",
       "      <td>107313</td>\n",
       "      <td>30.3</td>\n",
       "      <td>12088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>6612</td>\n",
       "      <td>5206.2</td>\n",
       "      <td>107135</td>\n",
       "      <td>31.8</td>\n",
       "      <td>12059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>6116</td>\n",
       "      <td>4680.7</td>\n",
       "      <td>107701</td>\n",
       "      <td>28.5</td>\n",
       "      <td>10421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>6771</td>\n",
       "      <td>4861.5</td>\n",
       "      <td>107638</td>\n",
       "      <td>28.1</td>\n",
       "      <td>11554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>7490</td>\n",
       "      <td>3509.5</td>\n",
       "      <td>108467</td>\n",
       "      <td>28.2</td>\n",
       "      <td>12543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>6019</td>\n",
       "      <td>4364.2</td>\n",
       "      <td>109079</td>\n",
       "      <td>26.4</td>\n",
       "      <td>12132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>6771</td>\n",
       "      <td>3852.9</td>\n",
       "      <td>110242</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>6019</td>\n",
       "      <td>4089.0</td>\n",
       "      <td>110924</td>\n",
       "      <td>25.2</td>\n",
       "      <td>12132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>6890</td>\n",
       "      <td>3436.2</td>\n",
       "      <td>110999</td>\n",
       "      <td>28.5</td>\n",
       "      <td>12543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>5893</td>\n",
       "      <td>2971.5</td>\n",
       "      <td>109000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11554000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  show_low_hmi  total_arrests  crime_rate  population_estimates  \\\n",
       "0   2000             0           5893      3433.9                 97876   \n",
       "1   2001             0           6019      3403.9                 98549   \n",
       "2   2002             0           6116      3436.2                101358   \n",
       "3   2003             0           6021      3797.0                103220   \n",
       "4   2004             0           6609      3378.5                105020   \n",
       "5   2005             0           6365      3344.9                106968   \n",
       "6   2006             0           6612      2926.0                109000   \n",
       "7   2007             0           6890      2866.7                110999   \n",
       "8   2008             0           7490      3085.9                112348   \n",
       "9   2009             0           6771      3053.0                112975   \n",
       "10  2010             0           6116      3246.2                107697   \n",
       "11  2011             0           6771      2971.5                107552   \n",
       "12  2012             0           6116      5841.7                107313   \n",
       "13  2013             0           6612      5206.2                107135   \n",
       "14  2014             0           6116      4680.7                107701   \n",
       "15  2015             0           6771      4861.5                107638   \n",
       "16  2016             0           7490      3509.5                108467   \n",
       "17  2017             0           6019      4364.2                109079   \n",
       "18  2018             1           6771      3852.9                110242   \n",
       "19  2019             1           6019      4089.0                110924   \n",
       "20  2020             1           6890      3436.2                110999   \n",
       "21  2021             1           5893      2971.5                109000   \n",
       "\n",
       "    poverty_percentage  state_tax  \n",
       "0                 24.0    4246000  \n",
       "1                 24.0   12543000  \n",
       "2                 24.0   13005000  \n",
       "3                 21.0   12294000  \n",
       "4                 23.0   17950000  \n",
       "5                 28.0   12132000  \n",
       "6                 24.4   17950000  \n",
       "7                 23.4   12613000  \n",
       "8                 23.0   17950000  \n",
       "9                 27.4   10935000  \n",
       "10                27.8   12294000  \n",
       "11                32.4   12613000  \n",
       "12                30.3   12088000  \n",
       "13                31.8   12059000  \n",
       "14                28.5   10421000  \n",
       "15                28.1   11554000  \n",
       "16                28.2   12543000  \n",
       "17                26.4   12132000  \n",
       "18                28.0   13005000  \n",
       "19                25.2   12132000  \n",
       "20                28.5   12543000  \n",
       "21                28.0   11554000  "
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navajo['show_low_hmi'] = pd.cut(navajo.show_low_hmi, bins = [0, 199999, 600000], labels = ['0', '1'])\n",
    "navajo['show_low_hmi'] = navajo['show_low_hmi'].astype(int)\n",
    "navajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "8ad381dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = navajo.drop('show_low_hmi', axis = 1)\n",
    "Y = navajo['show_low_hmi']\n",
    "\n",
    "# Splitting training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "f8e6f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 11.6612 - accuracy: 0.2353 - val_loss: 15.2492 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "history = model.fit(x = X_train, y = y_train.values, batch_size = 4, epochs = 100, shuffle = True, verbose = 2, validation_data = (X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "id": "ff4b60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 18.18\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, accuracy = model.evaluate(X, Y, verbose = 0)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "245a1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A1EDA5DDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61939d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bcaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa85f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "ff47bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_284 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_285 (Dense)            (None, 36)                468       \n",
      "_________________________________________________________________\n",
      "dense_286 (Dense)            (None, 6)                 222       \n",
      "_________________________________________________________________\n",
      "dense_287 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 781\n",
      "Trainable params: 781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up activation, hidden, and output layers.\n",
    "model = Sequential([\n",
    "    Dense(units = 12, input_shape = (6,), activation = 'relu'),\n",
    "    Dense(units = 36, activation = 'relu'),\n",
    "    Dense(units = 6, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Layer summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "b998bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pima county model\n",
    "pima = pd.read_csv(\"C:/Users/edoar/Documents/School_Documents/Fall_2021/Capstone/random_forest_files/pima.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "876187f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tucson_hmi</th>\n",
       "      <th>total_arrests</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>population_estimates</th>\n",
       "      <th>poverty_percentage</th>\n",
       "      <th>state_tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>64739</td>\n",
       "      <td>7628.0</td>\n",
       "      <td>848521</td>\n",
       "      <td>13.0</td>\n",
       "      <td>237006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>67030</td>\n",
       "      <td>8131.8</td>\n",
       "      <td>865694</td>\n",
       "      <td>13.0</td>\n",
       "      <td>300972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>70134</td>\n",
       "      <td>8218.9</td>\n",
       "      <td>886063</td>\n",
       "      <td>14.0</td>\n",
       "      <td>244806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>70399</td>\n",
       "      <td>8330.8</td>\n",
       "      <td>903320</td>\n",
       "      <td>14.0</td>\n",
       "      <td>258376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>70070</td>\n",
       "      <td>8280.0</td>\n",
       "      <td>924205</td>\n",
       "      <td>15.0</td>\n",
       "      <td>279527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>68340</td>\n",
       "      <td>6565.8</td>\n",
       "      <td>948965</td>\n",
       "      <td>14.0</td>\n",
       "      <td>300972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>64130</td>\n",
       "      <td>4318.4</td>\n",
       "      <td>975476</td>\n",
       "      <td>15.3</td>\n",
       "      <td>321475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>68849</td>\n",
       "      <td>4101.5</td>\n",
       "      <td>996593</td>\n",
       "      <td>14.9</td>\n",
       "      <td>423443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>68105</td>\n",
       "      <td>3949.6</td>\n",
       "      <td>1009832</td>\n",
       "      <td>15.0</td>\n",
       "      <td>381861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>68679</td>\n",
       "      <td>3843.9</td>\n",
       "      <td>1020200</td>\n",
       "      <td>18.9</td>\n",
       "      <td>396239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>70399</td>\n",
       "      <td>3891.0</td>\n",
       "      <td>981620</td>\n",
       "      <td>17.8</td>\n",
       "      <td>423443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>68340</td>\n",
       "      <td>3797.4</td>\n",
       "      <td>988381</td>\n",
       "      <td>20.4</td>\n",
       "      <td>421622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>70134</td>\n",
       "      <td>3783.4</td>\n",
       "      <td>993052</td>\n",
       "      <td>19.9</td>\n",
       "      <td>408095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>64130</td>\n",
       "      <td>6210.9</td>\n",
       "      <td>997127</td>\n",
       "      <td>19.3</td>\n",
       "      <td>391642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>70134</td>\n",
       "      <td>5817.1</td>\n",
       "      <td>1004229</td>\n",
       "      <td>18.7</td>\n",
       "      <td>338441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>67030</td>\n",
       "      <td>6068.1</td>\n",
       "      <td>1009103</td>\n",
       "      <td>18.7</td>\n",
       "      <td>431371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>68340</td>\n",
       "      <td>5688.2</td>\n",
       "      <td>1016707</td>\n",
       "      <td>18.2</td>\n",
       "      <td>450466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>70134</td>\n",
       "      <td>5419.2</td>\n",
       "      <td>1026391</td>\n",
       "      <td>16.6</td>\n",
       "      <td>460684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>68849</td>\n",
       "      <td>4888.7</td>\n",
       "      <td>1036554</td>\n",
       "      <td>18.2</td>\n",
       "      <td>481585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>70399</td>\n",
       "      <td>3884.7</td>\n",
       "      <td>1047279</td>\n",
       "      <td>14.0</td>\n",
       "      <td>450466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>68340</td>\n",
       "      <td>3884.7</td>\n",
       "      <td>1026391</td>\n",
       "      <td>18.7</td>\n",
       "      <td>450466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>68679</td>\n",
       "      <td>3949.6</td>\n",
       "      <td>1047279</td>\n",
       "      <td>19.3</td>\n",
       "      <td>381861000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  tucson_hmi  total_arrests  crime_rate  population_estimates  \\\n",
       "0   2000           0          64739      7628.0                848521   \n",
       "1   2001           0          67030      8131.8                865694   \n",
       "2   2002           0          70134      8218.9                886063   \n",
       "3   2003           0          70399      8330.8                903320   \n",
       "4   2004           0          70070      8280.0                924205   \n",
       "5   2005           1          68340      6565.8                948965   \n",
       "6   2006           1          64130      4318.4                975476   \n",
       "7   2007           1          68849      4101.5                996593   \n",
       "8   2008           1          68105      3949.6               1009832   \n",
       "9   2009           0          68679      3843.9               1020200   \n",
       "10  2010           0          70399      3891.0                981620   \n",
       "11  2011           0          68340      3797.4                988381   \n",
       "12  2012           0          70134      3783.4                993052   \n",
       "13  2013           0          64130      6210.9                997127   \n",
       "14  2014           0          70134      5817.1               1004229   \n",
       "15  2015           0          67030      6068.1               1009103   \n",
       "16  2016           0          68340      5688.2               1016707   \n",
       "17  2017           0          70134      5419.2               1026391   \n",
       "18  2018           1          68849      4888.7               1036554   \n",
       "19  2019           1          70399      3884.7               1047279   \n",
       "20  2020           1          68340      3884.7               1026391   \n",
       "21  2021           1          68679      3949.6               1047279   \n",
       "\n",
       "    poverty_percentage  state_tax  \n",
       "0                 13.0  237006000  \n",
       "1                 13.0  300972000  \n",
       "2                 14.0  244806000  \n",
       "3                 14.0  258376000  \n",
       "4                 15.0  279527000  \n",
       "5                 14.0  300972000  \n",
       "6                 15.3  321475000  \n",
       "7                 14.9  423443000  \n",
       "8                 15.0  381861000  \n",
       "9                 18.9  396239000  \n",
       "10                17.8  423443000  \n",
       "11                20.4  421622000  \n",
       "12                19.9  408095000  \n",
       "13                19.3  391642000  \n",
       "14                18.7  338441000  \n",
       "15                18.7  431371000  \n",
       "16                18.2  450466000  \n",
       "17                16.6  460684000  \n",
       "18                18.2  481585000  \n",
       "19                14.0  450466000  \n",
       "20                18.7  450466000  \n",
       "21                19.3  381861000  "
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima['tucson_hmi'] = pd.cut(pima.tucson_hmi, bins = [0, 199999, 600000], labels = ['0', '1'])\n",
    "pima['tucson_hmi'] = pima['tucson_hmi'].astype(int)\n",
    "pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "3ac792cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = pima.drop('tucson_hmi', axis = 1)\n",
    "Y = pima['tucson_hmi']\n",
    "\n",
    "# Splitting training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "e28cdf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 9.8672 - accuracy: 0.3529 - val_loss: 9.1495 - val_accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "history = model.fit(x = X_train, y = y_train.values, batch_size = 4, epochs = 100, shuffle = True, verbose = 2, validation_data = (X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "2eccb003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.36\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, accuracy = model.evaluate(X, Y, verbose = 0)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "40b4ebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A1E6FC3A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7745966692414834"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f19e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a5562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e819ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "da3f5fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_288 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_289 (Dense)            (None, 36)                468       \n",
      "_________________________________________________________________\n",
      "dense_290 (Dense)            (None, 6)                 222       \n",
      "_________________________________________________________________\n",
      "dense_291 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 781\n",
      "Trainable params: 781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up activation, hidden, and output layers.\n",
    "model = Sequential([\n",
    "    Dense(units = 12, input_shape = (6,), activation = 'relu'),\n",
    "    Dense(units = 36, activation = 'relu'),\n",
    "    Dense(units = 6, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Layer summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "ab2a2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yavapai county model\n",
    "yavapai = pd.read_csv(\"C:/Users/edoar/Documents/School_Documents/Fall_2021/Capstone/random_forest_files/yavapai.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "3cc9027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>prescott_hmi</th>\n",
       "      <th>total_arrests</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>population_estimates</th>\n",
       "      <th>poverty_percentage</th>\n",
       "      <th>state_tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>6746</td>\n",
       "      <td>4790.6</td>\n",
       "      <td>168886</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>11993</td>\n",
       "      <td>4649.8</td>\n",
       "      <td>173285</td>\n",
       "      <td>13.0</td>\n",
       "      <td>28921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>11478</td>\n",
       "      <td>4587.3</td>\n",
       "      <td>178390</td>\n",
       "      <td>12.0</td>\n",
       "      <td>32497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>11082</td>\n",
       "      <td>4407.5</td>\n",
       "      <td>183400</td>\n",
       "      <td>12.0</td>\n",
       "      <td>36446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>12031</td>\n",
       "      <td>4274.2</td>\n",
       "      <td>189532</td>\n",
       "      <td>12.0</td>\n",
       "      <td>41321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12769</td>\n",
       "      <td>3985.0</td>\n",
       "      <td>197533</td>\n",
       "      <td>12.0</td>\n",
       "      <td>45088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>12927</td>\n",
       "      <td>3491.2</td>\n",
       "      <td>206672</td>\n",
       "      <td>12.4</td>\n",
       "      <td>50351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>12856</td>\n",
       "      <td>3321.6</td>\n",
       "      <td>212004</td>\n",
       "      <td>12.6</td>\n",
       "      <td>56254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>11863</td>\n",
       "      <td>3077.8</td>\n",
       "      <td>214930</td>\n",
       "      <td>13.0</td>\n",
       "      <td>45642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>11908</td>\n",
       "      <td>2912.6</td>\n",
       "      <td>215686</td>\n",
       "      <td>14.2</td>\n",
       "      <td>50382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>6746</td>\n",
       "      <td>3006.1</td>\n",
       "      <td>210983</td>\n",
       "      <td>18.3</td>\n",
       "      <td>53905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>12769</td>\n",
       "      <td>3038.8</td>\n",
       "      <td>211044</td>\n",
       "      <td>18.4</td>\n",
       "      <td>55056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>11993</td>\n",
       "      <td>2981.1</td>\n",
       "      <td>211990</td>\n",
       "      <td>15.7</td>\n",
       "      <td>51887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>12031</td>\n",
       "      <td>2880.8</td>\n",
       "      <td>214421</td>\n",
       "      <td>16.4</td>\n",
       "      <td>52131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>11908</td>\n",
       "      <td>2291.8</td>\n",
       "      <td>217716</td>\n",
       "      <td>15.6</td>\n",
       "      <td>45050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>11993</td>\n",
       "      <td>2872.3</td>\n",
       "      <td>220805</td>\n",
       "      <td>15.1</td>\n",
       "      <td>51335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>12031</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>224575</td>\n",
       "      <td>13.3</td>\n",
       "      <td>53116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>11908</td>\n",
       "      <td>2768.2</td>\n",
       "      <td>228082</td>\n",
       "      <td>13.9</td>\n",
       "      <td>54756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>12856</td>\n",
       "      <td>2313.0</td>\n",
       "      <td>231772</td>\n",
       "      <td>12.0</td>\n",
       "      <td>56254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>11993</td>\n",
       "      <td>2217.5</td>\n",
       "      <td>235099</td>\n",
       "      <td>12.0</td>\n",
       "      <td>51335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>11478</td>\n",
       "      <td>2872.3</td>\n",
       "      <td>228082</td>\n",
       "      <td>12.0</td>\n",
       "      <td>45050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>11863</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>235099</td>\n",
       "      <td>12.0</td>\n",
       "      <td>50382000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  prescott_hmi  total_arrests  crime_rate  population_estimates  \\\n",
       "0   2000             0           6746      4790.6                168886   \n",
       "1   2001             0          11993      4649.8                173285   \n",
       "2   2002             0          11478      4587.3                178390   \n",
       "3   2003             0          11082      4407.5                183400   \n",
       "4   2004             0          12031      4274.2                189532   \n",
       "5   2005             1          12769      3985.0                197533   \n",
       "6   2006             1          12927      3491.2                206672   \n",
       "7   2007             1          12856      3321.6                212004   \n",
       "8   2008             1          11863      3077.8                214930   \n",
       "9   2009             1          11908      2912.6                215686   \n",
       "10  2010             1           6746      3006.1                210983   \n",
       "11  2011             0          12769      3038.8                211044   \n",
       "12  2012             0          11993      2981.1                211990   \n",
       "13  2013             1          12031      2880.8                214421   \n",
       "14  2014             1          11908      2291.8                217716   \n",
       "15  2015             1          11993      2872.3                220805   \n",
       "16  2016             1          12031      2809.0                224575   \n",
       "17  2017             1          11908      2768.2                228082   \n",
       "18  2018             1          12856      2313.0                231772   \n",
       "19  2019             1          11993      2217.5                235099   \n",
       "20  2020             1          11478      2872.3                228082   \n",
       "21  2021             1          11863      2809.0                235099   \n",
       "\n",
       "    poverty_percentage  state_tax  \n",
       "0                 12.0   28921000  \n",
       "1                 13.0   28921000  \n",
       "2                 12.0   32497000  \n",
       "3                 12.0   36446000  \n",
       "4                 12.0   41321000  \n",
       "5                 12.0   45088000  \n",
       "6                 12.4   50351000  \n",
       "7                 12.6   56254000  \n",
       "8                 13.0   45642000  \n",
       "9                 14.2   50382000  \n",
       "10                18.3   53905000  \n",
       "11                18.4   55056000  \n",
       "12                15.7   51887000  \n",
       "13                16.4   52131000  \n",
       "14                15.6   45050000  \n",
       "15                15.1   51335000  \n",
       "16                13.3   53116000  \n",
       "17                13.9   54756000  \n",
       "18                12.0   56254000  \n",
       "19                12.0   51335000  \n",
       "20                12.0   45050000  \n",
       "21                12.0   50382000  "
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yavapai['prescott_hmi'] = pd.cut(yavapai.prescott_hmi, bins = [0, 199999, 600000], labels = ['0', '1'])\n",
    "yavapai['prescott_hmi'] = yavapai['prescott_hmi'].astype(int)\n",
    "yavapai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "64a38fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = yavapai.drop('prescott_hmi', axis = 1)\n",
    "Y = yavapai['prescott_hmi']\n",
    "\n",
    "# Splitting training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "6519ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 8/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 11/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 12/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 13/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 14/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 15/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 16/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 17/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 18/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 19/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 20/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 21/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 22/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 23/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 24/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 25/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 26/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 27/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 28/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 29/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 30/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 31/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 32/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 33/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 34/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 35/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 36/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 37/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 38/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 39/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 40/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 41/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 42/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 43/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 44/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 45/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 46/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 47/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 48/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 49/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 50/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 51/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 52/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 53/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 54/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 55/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 56/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 57/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 58/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 59/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 60/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 61/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 62/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 63/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 64/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 65/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 66/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 67/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 68/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 69/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 70/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 71/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 72/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 73/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 74/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 75/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 76/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 77/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 78/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 79/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 80/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 81/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 82/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 83/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 84/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 86/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 87/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 88/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 89/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 90/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 91/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 92/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 93/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 94/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 95/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 96/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 97/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 98/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 99/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n",
      "Epoch 100/100\n",
      "5/5 - 0s - loss: 3.5881 - accuracy: 0.7647 - val_loss: 9.1495 - val_accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "# Keras model\n",
    "history = model.fit(x = X_train, y = y_train.values, batch_size = 4, epochs = 100, shuffle = True, verbose = 2, validation_data = (X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "b73b28c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.18\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy\n",
    "_, accuracy = model.evaluate(X, Y, verbose = 0)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "b97a0d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A1E92D8C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7745966692414834"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "predictions = model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
